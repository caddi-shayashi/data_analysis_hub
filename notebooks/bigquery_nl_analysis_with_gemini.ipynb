{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigQuery Natural Language Analysis with Gemini\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€Gemini APIã‚’ä½¿ç”¨ã—ã¦BigQueryã®Salesforceãƒ‡ãƒ¼ã‚¿ã‚’è‡ªç„¶è¨€èªã§åˆ†æã—ã¾ã™ã€‚\n",
    "\n",
    "## ä¸»ãªæ©Ÿèƒ½\n",
    "- ğŸ—£ï¸ **è‡ªç„¶è¨€èªã§ã®ã‚¯ã‚¨ãƒª**: æ—¥æœ¬èªã§ãƒ‡ãƒ¼ã‚¿ã«é–¢ã™ã‚‹è³ªå•ã‚’ã™ã‚‹ã ã‘ã§åˆ†æå¯èƒ½\n",
    "- ğŸ¤– **è‡ªå‹•SQLç”Ÿæˆ**: Gemini APIãŒã‚¹ã‚­ãƒ¼ãƒã‚’ç†è§£ã—ã¦SQLã‚’ç”Ÿæˆ\n",
    "- ğŸ“Š **çµæœã®å¯è¦–åŒ–**: Plotlyã«ã‚ˆã‚‹è‡ªå‹•ã‚°ãƒ©ãƒ•ç”Ÿæˆ\n",
    "- ğŸ“ **ã‚µãƒãƒªãƒ¼ç”Ÿæˆ**: AIã«ã‚ˆã‚‹åˆ†æçµæœã®è¦ç´„\n",
    "- ğŸ“ˆ **ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**: è¤‡æ•°åˆ†æã®çµ±åˆè¡¨ç¤º\n",
    "- ğŸ’¾ **å±¥æ­´ç®¡ç†**: åˆ†æå±¥æ­´ã®ä¿å­˜ã¨å‚ç…§\n",
    "\n",
    "## ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆ2025å¹´1æœˆæœ€æ–°ï¼‰\n",
    "- **Gemini 2.5 Pro** (gemini-2.5-pro-exp-0117): ãƒ‡ãƒ¼ã‚¿åˆ†ææœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«\n",
    "- **Gemini 2.5 Flash** (gemini-2.5-flash-exp-0117): é«˜é€Ÿãƒãƒ©ãƒ³ã‚¹ç‰ˆ\n",
    "- **Gemini 2.0 Flash** (gemini-2.0-flash-exp-01-21): è¶…é«˜é€Ÿå‡¦ç†ç‰ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­å®šã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install -q google-cloud-bigquery google-cloud-bigquery-storage pandas db-dtypes\n",
    "!pip install -q google-generativeai\n",
    "!pip install -q plotly matplotlib seaborn\n",
    "!pip install -q pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Google Cloudèªè¨¼ã¨APIè¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Google Cloudèªè¨¼ï¼ˆColabç’°å¢ƒã®å ´åˆï¼‰\n",
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    print('âœ… Google Cloudèªè¨¼å®Œäº†')\n",
    "except ImportError:\n",
    "    print('â„¹ï¸ ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œä¸­ - gcloud auth ã‚’ä½¿ç”¨')\n",
    "\n",
    "# BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import GoogleCloudError\n",
    "\n",
    "PROJECT_ID = 'esperanto-drawer-prod'\n",
    "DATASET_ID = 'dm_business_planning'\n",
    "\n",
    "try:\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    print(f'âœ… BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†: {PROJECT_ID}')\n",
    "except Exception as e:\n",
    "    print(f'âŒ BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}')\n",
    "    client = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gemini APIè¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Gemini API ã‚­ãƒ¼ã®è¨­å®š\n",
    "def setup_gemini_api():\n",
    "    \"\"\"Gemini APIã‚­ãƒ¼ã‚’è¨­å®šã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–\"\"\"\n",
    "    api_key = None\n",
    "    \n",
    "    # 1. Colabã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‹ã‚‰å–å¾—ã‚’è©¦ã¿ã‚‹\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        api_key = userdata.get('GEMINI_API_KEY')\n",
    "        print('âœ… Colabã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‹ã‚‰ API Key ã‚’å–å¾—ã—ã¾ã—ãŸ')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # 2. ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã‚’è©¦ã¿ã‚‹\n",
    "    if not api_key:\n",
    "        api_key = os.getenv('GEMINI_API_KEY')\n",
    "        if api_key:\n",
    "            print('âœ… ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ API Key ã‚’å–å¾—ã—ã¾ã—ãŸ')\n",
    "    \n",
    "    # 3. æ‰‹å‹•å…¥åŠ›\n",
    "    if not api_key:\n",
    "        from getpass import getpass\n",
    "        api_key = getpass('Gemini API Key ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ')\n",
    "    \n",
    "    genai.configure(api_key=api_key)\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰\n",
    "    models_to_try = [\n",
    "        ('gemini-2.5-pro-exp-0117', 'æœ€æ–°ãƒ‡ãƒ¼ã‚¿åˆ†æç‰¹åŒ–ç‰ˆ'),\n",
    "        ('gemini-2.5-flash-exp-0117', 'é«˜é€Ÿãƒãƒ©ãƒ³ã‚¹ç‰ˆ'),\n",
    "        ('gemini-2.0-flash-exp-01-21', 'è¶…é«˜é€Ÿç‰ˆ'),\n",
    "        ('gemini-1.5-pro', 'å®‰å®šç‰ˆ')\n",
    "    ]\n",
    "    \n",
    "    for model_name, description in models_to_try:\n",
    "        try:\n",
    "            model = genai.GenerativeModel(model_name)\n",
    "            print(f'âœ… ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_name} ({description})')\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f'âš ï¸ {model_name} ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“: {e}')\n",
    "            continue\n",
    "    \n",
    "    raise Exception(\"åˆ©ç”¨å¯èƒ½ãªGeminiãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "\n",
    "# Geminiãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–\n",
    "model = setup_gemini_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ¼ãƒã®å–å¾—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_schema_safe(dataset_id: str, table_id: str) -> pd.DataFrame:\n",
    "    \"\"\"ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¹ã‚­ãƒ¼ãƒæƒ…å ±ã‚’å®‰å…¨ã«å–å¾—\"\"\"\n",
    "    \n",
    "    # æœ€ã‚‚åŸºæœ¬çš„ãªã‚¯ã‚¨ãƒªã‹ã‚‰å§‹ã‚ã‚‹\n",
    "    queries = [\n",
    "        # Option 1: COLUMN_FIELD_PATHSã‚’ä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰\n",
    "        f\"\"\"\n",
    "        SELECT \n",
    "            column_name,\n",
    "            data_type,\n",
    "            field_path\n",
    "        FROM `{PROJECT_ID}.{dataset_id}.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS`\n",
    "        WHERE table_name = '{table_id}'\n",
    "        ORDER BY column_name\n",
    "        \"\"\",\n",
    "        # Option 2: COLUMNSã‚’ä½¿ç”¨ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰\n",
    "        f\"\"\"\n",
    "        SELECT \n",
    "            column_name,\n",
    "            data_type\n",
    "        FROM `{PROJECT_ID}.{dataset_id}.INFORMATION_SCHEMA.COLUMNS`\n",
    "        WHERE table_name = '{table_id}'\n",
    "        ORDER BY column_name\n",
    "        \"\"\",\n",
    "        # Option 3: ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ç›´æ¥ã‚¹ã‚­ãƒ¼ãƒã‚’å–å¾—ï¼ˆæœ€çµ‚æ‰‹æ®µï¼‰\n",
    "        f\"\"\"\n",
    "        SELECT *\n",
    "        FROM `{PROJECT_ID}.{dataset_id}.{table_id}`\n",
    "        LIMIT 0\n",
    "        \"\"\"\n",
    "    ]\n",
    "    \n",
    "    for i, query in enumerate(queries, 1):\n",
    "        try:\n",
    "            print(f\"è©¦è¡Œ {i}/3: ã‚¹ã‚­ãƒ¼ãƒå–å¾—ä¸­...\")\n",
    "            if i == 3:\n",
    "                # Option 3ã®å ´åˆã¯ã€ã‚¯ã‚¨ãƒªçµæœã‹ã‚‰ã‚¹ã‚­ãƒ¼ãƒã‚’æ¨å®š\n",
    "                job = client.query(query)\n",
    "                schema = []\n",
    "                for field in job.schema:\n",
    "                    schema.append({\n",
    "                        'column_name': field.name,\n",
    "                        'data_type': field.field_type,\n",
    "                        'description': field.description or ''\n",
    "                    })\n",
    "                df = pd.DataFrame(schema)\n",
    "            else:\n",
    "                df = client.query(query).to_dataframe()\n",
    "            \n",
    "            # descriptionã‚«ãƒ©ãƒ ãŒãªã„å ´åˆã¯è¿½åŠ \n",
    "            if 'description' not in df.columns:\n",
    "                df['description'] = ''\n",
    "            \n",
    "            print(f\"âœ… ã‚¹ã‚­ãƒ¼ãƒå–å¾—æˆåŠŸ: {len(df)} ã‚«ãƒ©ãƒ \")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ è©¦è¡Œ {i} å¤±æ•—: {str(e)[:100]}\")\n",
    "            continue\n",
    "    \n",
    "    # ã™ã¹ã¦å¤±æ•—ã—ãŸå ´åˆ\n",
    "    print(\"âŒ ã‚¹ã‚­ãƒ¼ãƒå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "    return pd.DataFrame(columns=['column_name', 'data_type', 'description'])\n",
    "\n",
    "# ã‚¹ã‚­ãƒ¼ãƒæƒ…å ±ã‚’å–å¾—\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ“Š Salesforce Account Mart ã‚¹ã‚­ãƒ¼ãƒå–å¾—\")\n",
    "account_schema = get_table_schema_safe(DATASET_ID, 'salesforce_account_mart')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ“Š Salesforce Opportunity Mart ã‚¹ã‚­ãƒ¼ãƒå–å¾—\")\n",
    "opportunity_schema = get_table_schema_safe(DATASET_ID, 'salesforce_opportunity_mart')\n",
    "\n",
    "# çµæœè¡¨ç¤º\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"âœ… Account Mart: {len(account_schema)} ã‚«ãƒ©ãƒ \")\n",
    "print(f\"âœ… Opportunity Mart: {len(opportunity_schema)} ã‚«ãƒ©ãƒ \")\n",
    "\n",
    "# ä¸»è¦ã‚«ãƒ©ãƒ ã‚’è¡¨ç¤º\n",
    "if not account_schema.empty:\n",
    "    print(\"\\nğŸ“‹ Account Mart ä¸»è¦ã‚«ãƒ©ãƒ :\")\n",
    "    display(account_schema.head(10))\n",
    "\n",
    "if not opportunity_schema.empty:\n",
    "    print(\"\\nğŸ“‹ Opportunity Mart ä¸»è¦ã‚«ãƒ©ãƒ :\")\n",
    "    display(opportunity_schema.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. è‡ªç„¶è¨€èªâ†’SQLå¤‰æ›ã‚¯ãƒ©ã‚¹ï¼ˆã‚³ã‚¢æ©Ÿèƒ½ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLToBigQueryAnalyzer:\n",
    "    \"\"\"è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒªã‚’BigQuery SQLã«å¤‰æ›ã—ã¦å®Ÿè¡Œã™ã‚‹ã‚¯ãƒ©ã‚¹\"\"\"\n",
    "    \n",
    "    def __init__(self, client: bigquery.Client, model, \n",
    "                 account_schema: pd.DataFrame, \n",
    "                 opportunity_schema: pd.DataFrame):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.account_schema = account_schema\n",
    "        self.opportunity_schema = opportunity_schema\n",
    "        self.history = []  # åˆ†æå±¥æ­´\n",
    "        \n",
    "    def generate_sql(self, natural_language_query: str) -> str:\n",
    "        \"\"\"è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒªã‹ã‚‰SQLã‚’ç”Ÿæˆ\"\"\"\n",
    "        \n",
    "        # ã‚¹ã‚­ãƒ¼ãƒæƒ…å ±ã‚’æ–‡å­—åˆ—åŒ–ï¼ˆæœ€å¤§50ã‚«ãƒ©ãƒ ãšã¤ï¼‰\n",
    "        account_cols = self.account_schema[['column_name', 'data_type']].head(50).to_string(index=False)\n",
    "        opportunity_cols = self.opportunity_schema[['column_name', 'data_type']].head(50).to_string(index=False)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        ã‚ãªãŸã¯BigQueryã®SQLã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚\n",
    "        ä»¥ä¸‹ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ¼ãƒã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã‚‹SQLã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n",
    "        \n",
    "        ## åˆ©ç”¨å¯èƒ½ãªãƒ†ãƒ¼ãƒ–ãƒ«:\n",
    "        \n",
    "        ### 1. `esperanto-drawer-prod.dm_business_planning.salesforce_account_mart`\n",
    "        ä¼æ¥­ãƒ»é¡§å®¢ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿\n",
    "        ä¸»è¦ã‚«ãƒ©ãƒ :\n",
    "        {account_cols}\n",
    "        \n",
    "        ### 2. `esperanto-drawer-prod.dm_business_planning.salesforce_opportunity_mart`\n",
    "        å•†è«‡ãƒ»æ¡ˆä»¶ãƒ‡ãƒ¼ã‚¿\n",
    "        ä¸»è¦ã‚«ãƒ©ãƒ :\n",
    "        {opportunity_cols}\n",
    "        \n",
    "        ## é‡è¦ãªæ³¨æ„äº‹é …:\n",
    "        - æ—¥ä»˜ã‚«ãƒ©ãƒ ã¯ TIMESTAMP ã¾ãŸã¯ DATE å‹\n",
    "        - é‡‘é¡ã‚«ãƒ©ãƒ ã¯ Amount, AnnualRevenue ãªã©\n",
    "        - ã‚¹ãƒ†ãƒ¼ã‚¸åã¯ StageName\n",
    "        - å¿…ãš LIMIT 1000 ã‚’ä»˜ã‘ã‚‹ï¼ˆå¤§é‡ãƒ‡ãƒ¼ã‚¿é˜²æ­¢ï¼‰\n",
    "        - JOINã™ã‚‹å ´åˆã¯ AccountId ã§çµåˆ\n",
    "        - ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã«ãã„ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¯ã‚¨ãƒªã‚’å¿ƒãŒã‘ã‚‹\n",
    "        \n",
    "        ## ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:\n",
    "        {natural_language_query}\n",
    "        \n",
    "        SQLã‚¯ã‚¨ãƒªã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚\n",
    "        SQLã¯```sql ã¨ ``` ã§å›²ã‚“ã§ãã ã•ã„ã€‚\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.model.generate_content(prompt)\n",
    "            sql = response.text\n",
    "            \n",
    "            # SQLã‚’æŠ½å‡º\n",
    "            if '```sql' in sql:\n",
    "                sql = sql.split('```sql')[1].split('```')[0].strip()\n",
    "            elif '```' in sql:\n",
    "                sql = sql.split('```')[1].split('```')[0].strip()\n",
    "                \n",
    "            return sql\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ SQLç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def execute_query(self, sql: str) -> Tuple[pd.DataFrame, str]:\n",
    "        \"\"\"SQLã‚’å®Ÿè¡Œã—ã¦DataFrameã‚’è¿”ã™\"\"\"\n",
    "        try:\n",
    "            # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ã§ã‚³ã‚¹ãƒˆç¢ºèª\n",
    "            job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "            dry_run_job = self.client.query(sql, job_config=job_config)\n",
    "            \n",
    "            bytes_processed = dry_run_job.total_bytes_processed\n",
    "            gb_processed = bytes_processed / 1e9\n",
    "            cost_estimate = gb_processed * 5  # $5 per TB\n",
    "            \n",
    "            print(f\"ğŸ’° å‡¦ç†äºˆå®š: {gb_processed:.3f} GB (æ¨å®šã‚³ã‚¹ãƒˆ: ${cost_estimate:.4f})\")\n",
    "            \n",
    "            # å®Ÿéš›ã«å®Ÿè¡Œ\n",
    "            query_job = self.client.query(sql)\n",
    "            df = query_job.to_dataframe()\n",
    "            \n",
    "            return df, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"âŒ ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {error_msg[:200]}\")\n",
    "            return pd.DataFrame(), error_msg\n",
    "    \n",
    "    def generate_summary(self, query: str, df: pd.DataFrame) -> str:\n",
    "        \"\"\"çµæœã®ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ\"\"\"\n",
    "        if df.empty:\n",
    "            return \"ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\"\n",
    "        \n",
    "        # DataFrameã®çµ±è¨ˆæƒ…å ±\n",
    "        stats = {\n",
    "            'è¡Œæ•°': len(df),\n",
    "            'ã‚«ãƒ©ãƒ æ•°': len(df.columns),\n",
    "            'ã‚«ãƒ©ãƒ å': list(df.columns)[:10]  # æœ€åˆã®10ã‚«ãƒ©ãƒ \n",
    "        }\n",
    "        \n",
    "        # æ•°å€¤ã‚«ãƒ©ãƒ ã®åŸºæœ¬çµ±è¨ˆ\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            stats['æ•°å€¤ã‚«ãƒ©ãƒ '] = list(numeric_cols)[:5]\n",
    "        \n",
    "        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€å¤§10è¡Œï¼‰\n",
    "        sample_data = df.head(10).to_string() if len(df) > 0 else \"ãƒ‡ãƒ¼ã‚¿ãªã—\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿åˆ†æçµæœã‚’æ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n",
    "        \n",
    "        å…ƒã®è³ªå•: {query}\n",
    "        \n",
    "        ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:\n",
    "        {json.dumps(stats, ensure_ascii=False, indent=2)}\n",
    "        \n",
    "        ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€å¤§10è¡Œï¼‰:\n",
    "        {sample_data[:2000]}\n",
    "        \n",
    "        ä»¥ä¸‹ã®è¦³ç‚¹ã§3-5æ–‡ã§è¦ç´„ã—ã¦ãã ã•ã„ï¼š\n",
    "        1. ä¸»è¦ãªç™ºè¦‹ãƒ»ã‚¤ãƒ³ã‚µã‚¤ãƒˆ\n",
    "        2. æ•°å€¤çš„ãªå‚¾å‘\n",
    "        3. ãƒ“ã‚¸ãƒã‚¹ä¸Šã®ç¤ºå”†\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.model.generate_content(prompt)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            return f\"ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}\"\n",
    "    \n",
    "    def analyze(self, natural_language_query: str, show_details: bool = True) -> Dict:\n",
    "        \"\"\"è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒªã‚’åˆ†æã—ã¦çµæœã‚’è¿”ã™\"\"\"\n",
    "        \n",
    "        result = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'query': natural_language_query,\n",
    "            'sql': None,\n",
    "            'data': pd.DataFrame(),\n",
    "            'error': None,\n",
    "            'summary': None\n",
    "        }\n",
    "        \n",
    "        if show_details:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"ğŸ” è³ªå•: {natural_language_query}\")\n",
    "            print(\"='*60}\")\n",
    "        \n",
    "        # SQLç”Ÿæˆ\n",
    "        print(\"\\nâ³ SQLç”Ÿæˆä¸­...\")\n",
    "        sql = self.generate_sql(natural_language_query)\n",
    "        if not sql:\n",
    "            result['error'] = \"SQLç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ\"\n",
    "            return result\n",
    "        \n",
    "        result['sql'] = sql\n",
    "        if show_details:\n",
    "            print(f\"\\nğŸ“ ç”Ÿæˆã•ã‚ŒãŸSQL:\\n{sql}\")\n",
    "        \n",
    "        # SQLå®Ÿè¡Œ\n",
    "        print(\"\\nâ³ ã‚¯ã‚¨ãƒªå®Ÿè¡Œä¸­...\")\n",
    "        df, error = self.execute_query(sql)\n",
    "        \n",
    "        if error:\n",
    "            result['error'] = error\n",
    "            result['summary'] = f\"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error[:100]}\"\n",
    "        else:\n",
    "            result['data'] = df\n",
    "            print(f\"âœ… {len(df)} è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\")\n",
    "            \n",
    "            # ã‚µãƒãƒªãƒ¼ç”Ÿæˆ\n",
    "            print(\"\\nâ³ ã‚µãƒãƒªãƒ¼ç”Ÿæˆä¸­...\")\n",
    "            result['summary'] = self.generate_summary(natural_language_query, df)\n",
    "        \n",
    "        # å±¥æ­´ã«è¿½åŠ \n",
    "        self.history.append(result)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ\n",
    "if client and model:\n",
    "    analyzer = NLToBigQueryAnalyzer(client, model, account_schema, opportunity_schema)\n",
    "    print(\"\\nâœ… ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–å®Œäº†\")\n",
    "else:\n",
    "    print(\"\\nâŒ ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–å¤±æ•—ï¼ˆBigQueryã¾ãŸã¯Gemini APIã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰\")\n",
    "    analyzer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. åˆ†æå®Ÿè¡Œã¨å¯è¦–åŒ–é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def run_analysis(question: str, show_chart: bool = True, chart_type: str = 'auto') -> Dict:\n",
    "    \"\"\"è‡ªç„¶è¨€èªã§åˆ†æã‚’å®Ÿè¡Œã—ã€çµæœã‚’å¯è¦–åŒ–\"\"\"\n",
    "    \n",
    "    if not analyzer:\n",
    "        print(\"âŒ ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "        return None\n",
    "    \n",
    "    # åˆ†æå®Ÿè¡Œ\n",
    "    result = analyzer.analyze(question)\n",
    "    \n",
    "    # çµæœè¡¨ç¤º\n",
    "    if result['summary']:\n",
    "        print(f\"\\nğŸ“Š åˆ†æçµæœã‚µãƒãƒªãƒ¼:\")\n",
    "        print(result['summary'])\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º\n",
    "    if not result['data'].empty:\n",
    "        print(f\"\\nğŸ“‹ ãƒ‡ãƒ¼ã‚¿ï¼ˆä¸Šä½10è¡Œï¼‰:\")\n",
    "        display(result['data'].head(10))\n",
    "        \n",
    "        # ã‚°ãƒ©ãƒ•è¡¨ç¤º\n",
    "        if show_chart:\n",
    "            create_visualization(result['data'], question, chart_type)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_visualization(df: pd.DataFrame, title: str, chart_type: str = 'auto'):\n",
    "    \"\"\"ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰é©åˆ‡ãªã‚°ãƒ©ãƒ•ã‚’è‡ªå‹•ç”Ÿæˆ\"\"\"\n",
    "    \n",
    "    if df.empty:\n",
    "        return\n",
    "    \n",
    "    # æ•°å€¤ã‚«ãƒ©ãƒ ã¨éæ•°å€¤ã‚«ãƒ©ãƒ ã‚’è­˜åˆ¥\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    \n",
    "    # chart_typeãŒ'auto'ã®å ´åˆã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ€é©ãªã‚°ãƒ©ãƒ•ã‚¿ã‚¤ãƒ—ã‚’æ¨å®š\n",
    "    if chart_type == 'auto':\n",
    "        if len(df) <= 20 and len(numeric_cols) > 0:\n",
    "            chart_type = 'bar'\n",
    "        elif len(numeric_cols) >= 2:\n",
    "            chart_type = 'scatter'\n",
    "        elif len(numeric_cols) == 1 and len(df) > 20:\n",
    "            chart_type = 'line'\n",
    "        else:\n",
    "            chart_type = 'table'\n",
    "    \n",
    "    try:\n",
    "        if chart_type == 'bar' and len(numeric_cols) > 0:\n",
    "            # æ£’ã‚°ãƒ©ãƒ•\n",
    "            x_col = non_numeric_cols[0] if non_numeric_cols else df.index\n",
    "            y_col = numeric_cols[0]\n",
    "            fig = px.bar(df.head(20), x=x_col, y=y_col, title=title)\n",
    "            \n",
    "        elif chart_type == 'line' and len(numeric_cols) > 0:\n",
    "            # ç·šã‚°ãƒ©ãƒ•\n",
    "            x_col = non_numeric_cols[0] if non_numeric_cols else df.index\n",
    "            y_col = numeric_cols[0]\n",
    "            fig = px.line(df, x=x_col, y=y_col, title=title, markers=True)\n",
    "            \n",
    "        elif chart_type == 'scatter' and len(numeric_cols) >= 2:\n",
    "            # æ•£å¸ƒå›³\n",
    "            fig = px.scatter(df, x=numeric_cols[0], y=numeric_cols[1], title=title)\n",
    "            \n",
    "        elif chart_type == 'pie' and len(numeric_cols) > 0 and len(non_numeric_cols) > 0:\n",
    "            # å††ã‚°ãƒ©ãƒ•\n",
    "            fig = px.pie(df.head(10), values=numeric_cols[0], names=non_numeric_cols[0], title=title)\n",
    "            \n",
    "        else:\n",
    "            # ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º\n",
    "            fig = go.Figure(data=[go.Table(\n",
    "                header=dict(values=list(df.columns),\n",
    "                           fill_color='paleturquoise',\n",
    "                           align='left'),\n",
    "                cells=dict(values=[df[col] for col in df.columns],\n",
    "                          fill_color='lavender',\n",
    "                          align='left'))\n",
    "            ])\n",
    "            fig.update_layout(title=title)\n",
    "        \n",
    "        fig.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ã‚°ãƒ©ãƒ•ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "print(\"âœ… åˆ†æãƒ»å¯è¦–åŒ–é–¢æ•°ã®æº–å‚™å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”Ÿæˆæ©Ÿèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dashboard(analyses: Dict[str, str] = None):\n",
    "    \"\"\"è¤‡æ•°ã®åˆ†æçµæœã‚’çµ„ã¿åˆã‚ã›ãŸãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’ä½œæˆ\"\"\"\n",
    "    \n",
    "    if not analyzer:\n",
    "        print(\"âŒ ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "        return None\n",
    "    \n",
    "    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®åˆ†æã‚»ãƒƒãƒˆ\n",
    "    if analyses is None:\n",
    "        analyses = {\n",
    "            \"æœˆåˆ¥å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰\": \"2024å¹´ã®æœˆåˆ¥å—æ³¨é‡‘é¡ã‚’é›†è¨ˆ\",\n",
    "            \"ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥å•†è«‡\": \"ç¾åœ¨ã®å•†è«‡ã‚’ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥ã«ä»¶æ•°ã¨é‡‘é¡ã§é›†è¨ˆ\",\n",
    "            \"é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ\": \"ENTã€MIDã€SMBãƒ•ãƒ©ã‚°åˆ¥ã®é¡§å®¢æ•°\",\n",
    "            \"æ¥­ç•Œåˆ¥åˆ†æ\": \"æ¥­ç•Œåˆ¥ã®é¡§å®¢æ•°ä¸Šä½10ä»¶\"\n",
    "        }\n",
    "    \n",
    "    results = {}\n",
    "    print(\"ğŸ“Š ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”Ÿæˆä¸­...\\n\")\n",
    "    \n",
    "    # å„åˆ†æã‚’å®Ÿè¡Œ\n",
    "    for title, query in analyses.items():\n",
    "        print(f\"â³ åˆ†æä¸­: {title}\")\n",
    "        try:\n",
    "            result = analyzer.analyze(query, show_details=False)\n",
    "            results[title] = result\n",
    "            if not result['data'].empty:\n",
    "                print(f\"âœ… {title}: {len(result['data'])} è¡Œå–å¾—\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ {title}: ãƒ‡ãƒ¼ã‚¿ãªã—\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {title}: ã‚¨ãƒ©ãƒ¼ - {e}\")\n",
    "            results[title] = None\n",
    "    \n",
    "    # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=list(analyses.keys()),\n",
    "        specs=[\n",
    "            [{'type': 'bar'}, {'type': 'bar'}],\n",
    "            [{'type': 'bar'}, {'type': 'scatter'}]\n",
    "        ],\n",
    "        vertical_spacing=0.15,\n",
    "        horizontal_spacing=0.15\n",
    "    )\n",
    "    \n",
    "    # å„ã‚°ãƒ©ãƒ•ã‚’é…ç½®\n",
    "    positions = [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "    \n",
    "    for (title, result), (row, col), color in zip(results.items(), positions, colors):\n",
    "        if result and not result['data'].empty:\n",
    "            df = result['data'].head(20)\n",
    "            \n",
    "            # ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "            \n",
    "            if len(numeric_cols) > 0:\n",
    "                y_col = numeric_cols[0]\n",
    "                x_col = non_numeric_cols[0] if len(non_numeric_cols) > 0 else df.index\n",
    "                \n",
    "                if row == 2 and col == 2:  # æœ€å¾Œã¯æ•£å¸ƒå›³/ç·šã‚°ãƒ©ãƒ•\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=df[x_col] if isinstance(x_col, str) else x_col,\n",
    "                            y=df[y_col],\n",
    "                            mode='lines+markers',\n",
    "                            name=title,\n",
    "                            marker=dict(color=color)\n",
    "                        ),\n",
    "                        row=row, col=col\n",
    "                    )\n",
    "                else:  # æ£’ã‚°ãƒ©ãƒ•\n",
    "                    fig.add_trace(\n",
    "                        go.Bar(\n",
    "                            x=df[x_col] if isinstance(x_col, str) else x_col,\n",
    "                            y=df[y_col],\n",
    "                            name=title,\n",
    "                            marker=dict(color=color)\n",
    "                        ),\n",
    "                        row=row, col=col\n",
    "                    )\n",
    "    \n",
    "    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨­å®š\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        showlegend=False,\n",
    "        title=dict(\n",
    "            text=\"<b>Salesforce ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰</b>\",\n",
    "            font=dict(size=20),\n",
    "            x=0.5,\n",
    "            xanchor='center'\n",
    "        ),\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    # è»¸ãƒ©ãƒ™ãƒ«ã‚’å›è»¢\n",
    "    fig.update_xaxes(tickangle=45)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"âœ… ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ©Ÿèƒ½ã®æº–å‚™å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. å±¥æ­´ç®¡ç†æ©Ÿèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from typing import List\n",
    "\n",
    "class AnalysisHistory:\n",
    "    \"\"\"åˆ†æå±¥æ­´ã®ç®¡ç†ã‚¯ãƒ©ã‚¹\"\"\"\n",
    "    \n",
    "    def __init__(self, analyzer: NLToBigQueryAnalyzer = None):\n",
    "        self.analyzer = analyzer\n",
    "        self.history = analyzer.history if analyzer else []\n",
    "    \n",
    "    def show_history(self, last_n: int = 10):\n",
    "        \"\"\"æœ€è¿‘ã®åˆ†æå±¥æ­´ã‚’è¡¨ç¤º\"\"\"\n",
    "        if not self.history:\n",
    "            print(\"ğŸ“­ å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nğŸ“š åˆ†æå±¥æ­´ï¼ˆæœ€æ–°{last_n}ä»¶ï¼‰\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, item in enumerate(self.history[-last_n:], 1):\n",
    "            print(f\"\\n[{i}] {item['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(f\"   è³ªå•: {item['query']}\")\n",
    "            if not item['data'].empty:\n",
    "                print(f\"   çµæœ: {len(item['data'])} è¡Œ Ã— {len(item['data'].columns)} ã‚«ãƒ©ãƒ \")\n",
    "            else:\n",
    "                print(f\"   çµæœ: ãƒ‡ãƒ¼ã‚¿ãªã—\")\n",
    "            if item['error']:\n",
    "                print(f\"   ã‚¨ãƒ©ãƒ¼: {item['error'][:100]}\")\n",
    "    \n",
    "    def get_history_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"å±¥æ­´ã‚’DataFrameã¨ã—ã¦å–å¾—\"\"\"\n",
    "        if not self.history:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        history_data = []\n",
    "        for item in self.history:\n",
    "            history_data.append({\n",
    "                'timestamp': item['timestamp'],\n",
    "                'query': item['query'],\n",
    "                'rows': len(item['data']) if not item['data'].empty else 0,\n",
    "                'columns': len(item['data'].columns) if not item['data'].empty else 0,\n",
    "                'has_error': item['error'] is not None\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(history_data)\n",
    "    \n",
    "    def save(self, filename: str = 'analysis_history.pkl'):\n",
    "        \"\"\"å±¥æ­´ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'wb') as f:\n",
    "                pickle.dump(self.history, f)\n",
    "            print(f\"ğŸ’¾ å±¥æ­´ã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    \n",
    "    def load(self, filename: str = 'analysis_history.pkl'):\n",
    "        \"\"\"å±¥æ­´ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'rb') as f:\n",
    "                self.history = pickle.load(f)\n",
    "            print(f\"ğŸ“‚ å±¥æ­´ã‚’ {filename} ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{len(self.history)}ä»¶ï¼‰\")\n",
    "            if self.analyzer:\n",
    "                self.analyzer.history = self.history\n",
    "        except FileNotFoundError:\n",
    "            print(f\"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ« {filename} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    \n",
    "    def export_to_csv(self, filename: str = 'analysis_history.csv'):\n",
    "        \"\"\"å±¥æ­´ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\"\"\"\n",
    "        df = self.get_history_dataframe()\n",
    "        if not df.empty:\n",
    "            df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"ğŸ“Š å±¥æ­´ã‚’ {filename} ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ\")\n",
    "        else:\n",
    "            print(\"ğŸ“­ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "\n",
    "# å±¥æ­´ç®¡ç†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ä½œæˆ\n",
    "if analyzer:\n",
    "    history_manager = AnalysisHistory(analyzer)\n",
    "    print(\"âœ… å±¥æ­´ç®¡ç†æ©Ÿèƒ½ã®æº–å‚™å®Œäº†\")\n",
    "else:\n",
    "    history_manager = None\n",
    "    print(\"âš ï¸ å±¥æ­´ç®¡ç†æ©Ÿèƒ½ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ï¼ˆã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ãŒæœªåˆæœŸåŒ–ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. åˆ†æä¾‹ã®å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æä¾‹1: å£²ä¸Šä¸Šä½ä¼æ¥­\n",
    "result1 = run_analysis(\"å¹´é–“å£²ä¸ŠãŒæœ€ã‚‚é«˜ã„ä¸Šä½10ç¤¾ã‚’æ•™ãˆã¦\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æä¾‹2: æœˆåˆ¥ãƒˆãƒ¬ãƒ³ãƒ‰\n",
    "result2 = run_analysis(\"2024å¹´ã®æœˆåˆ¥å•†è«‡æ•°ã®æ¨ç§»ã‚’è¦‹ã›ã¦\", chart_type='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æä¾‹3: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æ\n",
    "result3 = run_analysis(\"ENTã€MIDã€SMBãƒ•ãƒ©ã‚°åˆ¥ã®ä¼æ¥­æ•°ã¨å¹³å‡å£²ä¸Š\", chart_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‡ªç”±ã«è³ªå•ã‚’å…¥åŠ›ã—ã¦åˆ†æ\n",
    "question = input(\"åˆ†æã—ãŸã„å†…å®¹ã‚’è‡ªç„¶è¨€èªã§å…¥åŠ›ã—ã¦ãã ã•ã„: \")\n",
    "if question:\n",
    "    result = run_analysis(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚«ã‚¹ã‚¿ãƒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ç”Ÿæˆ\n",
    "dashboard_results = create_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. å±¥æ­´ã®ç®¡ç†ã¨è¡¨ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å±¥æ­´ã‚’è¡¨ç¤º\n",
    "if history_manager:\n",
    "    history_manager.show_history(last_n=5)\n",
    "    \n",
    "    # å±¥æ­´ã‚’DataFrameã¨ã—ã¦å–å¾—\n",
    "    history_df = history_manager.get_history_dataframe()\n",
    "    if not history_df.empty:\n",
    "        print(\"\\nğŸ“Š å±¥æ­´çµ±è¨ˆ:\")\n",
    "        display(history_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å±¥æ­´ã‚’ä¿å­˜\n",
    "if history_manager:\n",
    "    history_manager.save('analysis_history.pkl')\n",
    "    history_manager.export_to_csv('analysis_history.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. åˆ†æãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚ˆãä½¿ã†åˆ†æã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ\n",
    "analysis_templates = {\n",
    "    \"å–¶æ¥­ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹\": [\n",
    "        \"ä»Šæœˆã®ClosedWonã®å•†è«‡é‡‘é¡åˆè¨ˆã¯ï¼Ÿ\",\n",
    "        \"å•†è«‡ã®å¹³å‡ã‚¯ãƒ­ãƒ¼ã‚ºæœŸé–“ã‚’ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥ã«é›†è¨ˆ\",\n",
    "        \"ä»Šå››åŠæœŸã®æ–°è¦å•†è«‡æ•°ã¨é‡‘é¡\"\n",
    "    ],\n",
    "    \"é¡§å®¢åˆ†æ\": [\n",
    "        \"å¾“æ¥­å“¡æ•°1000äººä»¥ä¸Šã®å¤§ä¼æ¥­ãƒªã‚¹ãƒˆä¸Šä½20ç¤¾\",\n",
    "        \"æ¥­ç•Œåˆ¥ã®é¡§å®¢æ•°ã¨å¹³å‡å£²ä¸Š\",\n",
    "        \"åœ°åŸŸåˆ¥ã®é¡§å®¢åˆ†å¸ƒ\"\n",
    "    ],\n",
    "    \"ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ\": [\n",
    "        \"éå»12ãƒ¶æœˆã®æœˆåˆ¥å£²ä¸Šæ¨ç§»\",\n",
    "        \"å››åŠæœŸåˆ¥ã®æ–°è¦é¡§å®¢ç²å¾—æ•°\",\n",
    "        \"å•†è«‡ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥ã®æ¨ç§»\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé¸æŠã¨å®Ÿè¡Œ\n",
    "print(\"ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:\")\n",
    "for i, category in enumerate(analysis_templates.keys(), 1):\n",
    "    print(f\"{i}. {category}\")\n",
    "\n",
    "try:\n",
    "    choice = int(input(\"\\nã‚«ãƒ†ã‚´ãƒªç•ªå·ã‚’é¸æŠ (1-3): \"))\n",
    "    if 1 <= choice <= 3:\n",
    "        selected_category = list(analysis_templates.keys())[choice - 1]\n",
    "        print(f\"\\né¸æŠ: {selected_category}\")\n",
    "        \n",
    "        for question in analysis_templates[selected_category]:\n",
    "            print(f\"\\nâ–¶ {question}\")\n",
    "            try:\n",
    "                result = run_analysis(question, show_chart=True)\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "except:\n",
    "    print(\"ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã¾ã¨ã‚\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã¾ã—ãŸï¼š\n",
    "\n",
    "âœ… **å®Ÿè£…æ¸ˆã¿æ©Ÿèƒ½**\n",
    "1. ğŸ—£ï¸ **è‡ªç„¶è¨€èªã§ã®ã‚¯ã‚¨ãƒª** - æ—¥æœ¬èªã§ãƒ‡ãƒ¼ã‚¿ã«é–¢ã™ã‚‹è³ªå•ã‚’ã™ã‚‹ã ã‘ã§åˆ†æå¯èƒ½\n",
    "2. ğŸ¤– **è‡ªå‹•SQLç”Ÿæˆ** - Gemini APIãŒã‚¹ã‚­ãƒ¼ãƒã‚’ç†è§£ã—ã¦SQLã‚’ç”Ÿæˆ\n",
    "3. ğŸ“Š **çµæœã®å¯è¦–åŒ–** - Plotlyã«ã‚ˆã‚‹è‡ªå‹•ã‚°ãƒ©ãƒ•ç”Ÿæˆ\n",
    "4. ğŸ“ **ã‚µãƒãƒªãƒ¼ç”Ÿæˆ** - AIã«ã‚ˆã‚‹åˆ†æçµæœã®è¦ç´„\n",
    "5. ğŸ“ˆ **ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰** - è¤‡æ•°åˆ†æã®çµ±åˆè¡¨ç¤º\n",
    "6. ğŸ’¾ **å±¥æ­´ç®¡ç†** - åˆ†æå±¥æ­´ã®ä¿å­˜ã¨å‚ç…§\n",
    "\n",
    "### ä½¿ã„æ–¹ã®ãƒ’ãƒ³ãƒˆ\n",
    "- å…·ä½“çš„ãªæ•°å€¤ã‚„æœŸé–“ã‚’å«ã‚ã‚‹ã¨ã€ã‚ˆã‚Šæ­£ç¢ºãªåˆ†æãŒã§ãã¾ã™\n",
    "- ã€Œä¸Šä½10ä»¶ã€ã€Œæœˆåˆ¥ã€ã€Œæ¥­ç•Œåˆ¥ã€ãªã©ã®é›†è¨ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ã†ã¨åŠ¹æœçš„ã§ã™\n",
    "- è¤‡é›‘ãªåˆ†æã¯æ®µéšçš„ã«å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™\n",
    "\n",
    "### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n",
    "- **ã‚¹ã‚­ãƒ¼ãƒå–å¾—ã‚¨ãƒ©ãƒ¼**: INFORMATION_SCHEMAã®æ¨©é™ã‚’ç¢ºèª\n",
    "- **SQLå®Ÿè¡Œã‚¨ãƒ©ãƒ¼**: ã‚¯ã‚¨ãƒªã®æ§‹æ–‡ã¨ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’ç¢ºèª\n",
    "- **Gemini APIã‚¨ãƒ©ãƒ¼**: API ã‚­ãƒ¼ã¨åˆ©ç”¨åˆ¶é™ã‚’ç¢ºèª\n",
    "\n",
    "### æ³¨æ„äº‹é …\n",
    "- å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ã‚¨ãƒªã¯ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹ãŸã‚ã€LIMITå¥ãŒè‡ªå‹•è¿½åŠ ã•ã‚Œã¾ã™\n",
    "- Gemini APIã®åˆ©ç”¨åˆ¶é™ã«æ³¨æ„ã—ã¦ãã ã•ã„\n",
    "- æ©Ÿå¯†æƒ…å ±ã®å–ã‚Šæ‰±ã„ã«ã¯ååˆ†æ³¨æ„ã—ã¦ãã ã•ã„"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}