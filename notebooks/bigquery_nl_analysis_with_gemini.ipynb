{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigQuery Natural Language Analysis with Gemini\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€Gemini APIã‚’ä½¿ç”¨ã—ã¦BigQueryã®Salesforceãƒ‡ãƒ¼ã‚¿ã‚’è‡ªç„¶è¨€èªã§åˆ†æã—ã¾ã™ã€‚\n",
    "\n",
    "## ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆ2025å¹´1æœˆæœ€æ–°ï¼‰\n",
    "- **Gemini 2.5 Pro** (gemini-2.5-pro-exp-0117): ãƒ‡ãƒ¼ã‚¿åˆ†ææœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«\n",
    "  - æœ€å¤§200ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ã®é•·å¤§ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†\n",
    "  - é«˜ç²¾åº¦ãªSQLç”Ÿæˆã¨è¤‡é›‘ãªæ¨è«–\n",
    "  - å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸€æ‹¬åˆ†æ\n",
    "  - RAGï¼ˆRetrieval-Augmented Generationï¼‰å¯¾å¿œ\n",
    "  \n",
    "- ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«:\n",
    "  - **Gemini 2.5 Flash** (gemini-2.5-flash-exp-0117): é«˜é€Ÿãƒãƒ©ãƒ³ã‚¹ç‰ˆ\n",
    "  - **Gemini 2.0 Flash** (gemini-2.0-flash-exp-01-21): è¶…é«˜é€Ÿå‡¦ç†ç‰ˆ\n",
    "\n",
    "## å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«\n",
    "- `esperanto-drawer-prod.dm_business_planning.salesforce_account_mart`\n",
    "- `esperanto-drawer-prod.dm_business_planning.salesforce_opportunity_mart`\n",
    "\n",
    "## API Keyè¨­å®šæ–¹æ³•\n",
    "\n",
    "### Google Colabã®å ´åˆï¼ˆæ¨å¥¨ï¼‰\n",
    "1. Colabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®å·¦ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ã€ŒğŸ”‘ã€ï¼ˆã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆï¼‰ã‚¢ã‚¤ã‚³ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯\n",
    "2. ã€Œæ–°ã—ã„ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‚’è¿½åŠ ã€ã‚’ã‚¯ãƒªãƒƒã‚¯\n",
    "3. åå‰ã« `GEMINI_API_KEY` ã‚’å…¥åŠ›\n",
    "4. å€¤ã«Gemini API Keyã‚’è²¼ã‚Šä»˜ã‘\n",
    "5. ã€Œãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯ã€ã‚’ONã«ã™ã‚‹\n",
    "\n",
    "### ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®å ´åˆ\n",
    "ç’°å¢ƒå¤‰æ•° `GEMINI_API_KEY` ã«è¨­å®šã™ã‚‹ã‹ã€å®Ÿè¡Œæ™‚ã«å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­å®šã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. ç’°å¢ƒè¨­å®šã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Google Cloudèªè¨¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m auth\n\u001b[32m      2\u001b[39m auth.authenticate_user()\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mèªè¨¼å®Œäº†\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "print('èªè¨¼å®Œäº†')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. APIè¨­å®šã¨åˆæœŸåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.generativeai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcloud\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m bigquery\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenerativeai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgenai\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.generativeai'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud import bigquery\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Gemini API ã‚­ãƒ¼ã®è¨­å®š\n",
    "# Colabã®ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‹ã‚‰å–å¾— (Colabç’°å¢ƒã®å ´åˆ)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
    "    print('Colabã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‹ã‚‰ API Key ã‚’å–å¾—ã—ã¾ã—ãŸ')\n",
    "except:\n",
    "    # Colabã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆä»¥å¤–ã®ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—\n",
    "    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "    if not GEMINI_API_KEY:\n",
    "        # ç’°å¢ƒå¤‰æ•°ã«ã‚‚ãªã„å ´åˆã¯æ‰‹å‹•å…¥åŠ›\n",
    "        from getpass import getpass\n",
    "        GEMINI_API_KEY = getpass('Gemini API Key ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ')\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–\n",
    "PROJECT_ID = 'esperanto-drawer-prod'\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Geminiãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–\n",
    "# gemini-2.5-pro-exp-0117: 2025å¹´1æœˆæ™‚ç‚¹ã®æœ€æ–°ãƒ—ãƒ­ãƒ¢ãƒ‡ãƒ«\n",
    "# - æœ€å¤§200ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·\n",
    "# - ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»SQLç”Ÿæˆã«æœ€é©\n",
    "# - è¤‡é›‘ãªæ¨è«–ã¨é«˜ç²¾åº¦ãªåˆ†æãŒå¯èƒ½\n",
    "try:\n",
    "    model = genai.GenerativeModel('gemini-2.5-pro-exp-0117')\n",
    "    print('åˆæœŸåŒ–å®Œäº†')\n",
    "    print(f'ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: gemini-2.5-pro-exp-0117 (æœ€æ–°ãƒ‡ãƒ¼ã‚¿åˆ†æç‰¹åŒ–ç‰ˆ)')\n",
    "except:\n",
    "    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯1: 2.5 Flash (é«˜é€Ÿæ€§ã¨ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹)\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-2.5-flash-exp-0117')\n",
    "        print('åˆæœŸåŒ–å®Œäº†')\n",
    "        print(f'ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: gemini-2.5-flash-exp-0117 (é«˜é€Ÿãƒãƒ©ãƒ³ã‚¹ç‰ˆ)')\n",
    "    except:\n",
    "        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯2: 2.0 Flash (ã•ã‚‰ã«é«˜é€Ÿ)\n",
    "        model = genai.GenerativeModel('gemini-2.0-flash-exp-01-21')\n",
    "        print('åˆæœŸåŒ–å®Œäº†')\n",
    "        print(f'ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: gemini-2.0-flash-exp-01-21 (è¶…é«˜é€Ÿç‰ˆ)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ¼ãƒã®å–å¾—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_table_schema(dataset_id: str, table_id: str) -> pd.DataFrame:\n    \"\"\"ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¹ã‚­ãƒ¼ãƒæƒ…å ±ã‚’å–å¾—\"\"\"\n    # COLUMNS ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ç”¨ï¼ˆã‚ˆã‚Šæ¨™æº–çš„ï¼‰\n    query = f\"\"\"\n    SELECT \n        column_name,\n        data_type,\n        IFNULL(description, '') as description\n    FROM `{PROJECT_ID}.{dataset_id}.INFORMATION_SCHEMA.COLUMNS`\n    WHERE table_name = '{table_id}'\n    ORDER BY column_name\n    \"\"\"\n    try:\n        return client.query(query).to_dataframe()\n    except Exception as e:\n        print(f\"COLUMNSä½¿ç”¨æ™‚ã®ã‚¨ãƒ©ãƒ¼: {e}\")\n        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: COLUMN_FIELD_PATHSã‚’ä½¿ç”¨\n        query_fallback = f\"\"\"\n        SELECT \n            column_name,\n            data_type,\n            IFNULL(description, '') as description\n        FROM `{PROJECT_ID}.{dataset_id}.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS`\n        WHERE table_name = '{table_id}'\n        ORDER BY column_name\n        \"\"\"\n        return client.query(query_fallback).to_dataframe()\n\n# ã‚¹ã‚­ãƒ¼ãƒæƒ…å ±ã‚’å–å¾—\naccount_schema = get_table_schema('dm_business_planning', 'salesforce_account_mart')\nopportunity_schema = get_table_schema('dm_business_planning', 'salesforce_opportunity_mart')\n\nprint(\"Account Mart ã‚«ãƒ©ãƒ æ•°:\", len(account_schema))\nprint(\"Opportunity Mart ã‚«ãƒ©ãƒ æ•°:\", len(opportunity_schema))\n\n# ä¸»è¦ã‚«ãƒ©ãƒ ã‚’è¡¨ç¤º\nprint(\"\\n=== Account Mart ä¸»è¦ã‚«ãƒ©ãƒ  ===\")\ndisplay(account_schema.head(10))\n\nprint(\"\\n=== Opportunity Mart ä¸»è¦ã‚«ãƒ©ãƒ  ===\")\ndisplay(opportunity_schema.head(10))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. è‡ªç„¶è¨€èªâ†’SQLå¤‰æ›ã‚¯ãƒ©ã‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLToBigQueryAnalyzer:\n",
    "    def __init__(self, client: bigquery.Client, model):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.account_schema = account_schema\n",
    "        self.opportunity_schema = opportunity_schema\n",
    "        \n",
    "    def generate_sql(self, natural_language_query: str) -> str:\n",
    "        \"\"\"è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒªã‹ã‚‰SQLã‚’ç”Ÿæˆ\"\"\"\n",
    "        \n",
    "        # ã‚¹ã‚­ãƒ¼ãƒæƒ…å ±ã‚’æ–‡å­—åˆ—åŒ–\n",
    "        account_schema_str = self.account_schema[['column_name', 'data_type', 'description']].to_string()\n",
    "        opportunity_schema_str = self.opportunity_schema[['column_name', 'data_type', 'description']].to_string()\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        ã‚ãªãŸã¯BigQueryã®SQLã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚\n",
    "        ä»¥ä¸‹ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ¼ãƒã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã‚‹SQLã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n",
    "        \n",
    "        åˆ©ç”¨å¯èƒ½ãªãƒ†ãƒ¼ãƒ–ãƒ«:\n",
    "        1. `esperanto-drawer-prod.dm_business_planning.salesforce_account_mart`\n",
    "        ã‚¹ã‚­ãƒ¼ãƒ:\n",
    "        {account_schema_str[:2000]}  # æœ€åˆã®2000æ–‡å­—ã®ã¿\n",
    "        \n",
    "        2. `esperanto-drawer-prod.dm_business_planning.salesforce_opportunity_mart`\n",
    "        ã‚¹ã‚­ãƒ¼ãƒ:\n",
    "        {opportunity_schema_str[:2000]}  # æœ€åˆã®2000æ–‡å­—ã®ã¿\n",
    "        \n",
    "        é‡è¦ãªæ³¨æ„äº‹é …:\n",
    "        - æ—¥ä»˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ TIMESTAMP å‹ã§ã™\n",
    "        - é‡‘é¡ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ Amount, AnnualRevenue ãªã©ã§ã™\n",
    "        - ã‚¹ãƒ†ãƒ¼ã‚¸åã¯ StageName ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ã™\n",
    "        - æ—¥æœ¬èªã®ã‚«ãƒ©ãƒ èª¬æ˜ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„\n",
    "        - LIMIT 1000 ã‚’å¿…ãšä»˜ã‘ã¦ãã ã•ã„ï¼ˆå¤§é‡ãƒ‡ãƒ¼ã‚¿é˜²æ­¢ï¼‰\n",
    "        - JOINã™ã‚‹å ´åˆã¯ AccountId ã§çµåˆã—ã¦ãã ã•ã„\n",
    "        \n",
    "        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {natural_language_query}\n",
    "        \n",
    "        SQLã‚¯ã‚¨ãƒªã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚\n",
    "        SQLã¯```sql ã¨ ``` ã§å›²ã‚“ã§ãã ã•ã„ã€‚\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.model.generate_content(prompt)\n",
    "        \n",
    "        # SQLã‚’æŠ½å‡º\n",
    "        sql = response.text\n",
    "        if '```sql' in sql:\n",
    "            sql = sql.split('```sql')[1].split('```')[0].strip()\n",
    "        elif '```' in sql:\n",
    "            sql = sql.split('```')[1].split('```')[0].strip()\n",
    "            \n",
    "        return sql\n",
    "    \n",
    "    def execute_query(self, sql: str) -> pd.DataFrame:\n",
    "        \"\"\"SQLã‚’å®Ÿè¡Œã—ã¦DataFrameã‚’è¿”ã™\"\"\"\n",
    "        try:\n",
    "            # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³\n",
    "            job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "            dry_run_job = self.client.query(sql, job_config=job_config)\n",
    "            \n",
    "            # å‡¦ç†ãƒã‚¤ãƒˆæ•°ã‚’ç¢ºèª\n",
    "            print(f\"å‡¦ç†äºˆå®šãƒã‚¤ãƒˆæ•°: {dry_run_job.total_bytes_processed:,} bytes\")\n",
    "            print(f\"å‡¦ç†äºˆå®šGB: {dry_run_job.total_bytes_processed / 1e9:.2f} GB\")\n",
    "            \n",
    "            # å®Ÿéš›ã«å®Ÿè¡Œ\n",
    "            query_job = self.client.query(sql)\n",
    "            df = query_job.to_dataframe()\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def analyze(self, natural_language_query: str) -> Dict:\n",
    "        \"\"\"è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒªã‚’åˆ†æã—ã¦çµæœã‚’è¿”ã™\"\"\"\n",
    "        print(f\"è³ªå•: {natural_language_query}\")\n",
    "        print(\"\\nSQLç”Ÿæˆä¸­...\")\n",
    "        \n",
    "        # SQLç”Ÿæˆ\n",
    "        sql = self.generate_sql(natural_language_query)\n",
    "        print(f\"\\nç”Ÿæˆã•ã‚ŒãŸSQL:\\n{sql}\")\n",
    "        \n",
    "        # SQLå®Ÿè¡Œ\n",
    "        print(\"\\nSQLå®Ÿè¡Œä¸­...\")\n",
    "        df = self.execute_query(sql)\n",
    "        \n",
    "        if df.empty:\n",
    "            return {\n",
    "                'query': natural_language_query,\n",
    "                'sql': sql,\n",
    "                'data': df,\n",
    "                'summary': 'ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ'\n",
    "            }\n",
    "        \n",
    "        # çµæœã®ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ\n",
    "        summary = self.generate_summary(natural_language_query, df)\n",
    "        \n",
    "        return {\n",
    "            'query': natural_language_query,\n",
    "            'sql': sql,\n",
    "            'data': df,\n",
    "            'summary': summary\n",
    "        }\n",
    "    \n",
    "    def generate_summary(self, query: str, df: pd.DataFrame) -> str:\n",
    "        \"\"\"çµæœã®ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ\"\"\"\n",
    "        \n",
    "        # DataFrameã®æƒ…å ±ã‚’æ–‡å­—åˆ—åŒ–ï¼ˆæœ€å¤§100è¡Œï¼‰\n",
    "        df_str = df.head(100).to_string()\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿åˆ†æçµæœã‚’æ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n",
    "        \n",
    "        å…ƒã®è³ªå•: {query}\n",
    "        \n",
    "        çµæœãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€å¤§100è¡Œï¼‰:\n",
    "        {df_str[:3000]}  # æœ€åˆã®3000æ–‡å­—ã®ã¿\n",
    "        \n",
    "        ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:\n",
    "        - è¡Œæ•°: {len(df)}\n",
    "        - ã‚«ãƒ©ãƒ æ•°: {len(df.columns)}\n",
    "        \n",
    "        ä»¥ä¸‹ã®è¦³ç‚¹ã§è¦ç´„ã—ã¦ãã ã•ã„ï¼š\n",
    "        1. ä¸»è¦ãªç™ºè¦‹ãƒ»ã‚¤ãƒ³ã‚µã‚¤ãƒˆ\n",
    "        2. æ•°å€¤çš„ãªå‚¾å‘\n",
    "        3. ç‰¹ç­†ã™ã¹ããƒã‚¤ãƒ³ãƒˆ\n",
    "        \n",
    "        ç°¡æ½”ã«3-5æ–‡ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.model.generate_content(prompt)\n",
    "        return response.text\n",
    "\n",
    "# ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ\n",
    "analyzer = NLToBigQueryAnalyzer(client, model)\n",
    "print(\"ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. åˆ†æå®Ÿè¡Œé–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis(question: str, show_chart: bool = True):\n",
    "    \"\"\"è‡ªç„¶è¨€èªã§åˆ†æã‚’å®Ÿè¡Œ\"\"\"\n",
    "    \n",
    "    # åˆ†æå®Ÿè¡Œ\n",
    "    result = analyzer.analyze(question)\n",
    "    \n",
    "    # çµæœè¡¨ç¤º\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ“Š åˆ†æçµæœ\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"\\nğŸ“ ã‚µãƒãƒªãƒ¼:\\n{result['summary']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ï¼ˆä¸Šä½10è¡Œï¼‰:\")\n",
    "    display(result['data'].head(10))\n",
    "    \n",
    "    # ã‚°ãƒ©ãƒ•è¡¨ç¤ºï¼ˆå¯èƒ½ãªå ´åˆï¼‰\n",
    "    if show_chart and not result['data'].empty:\n",
    "        df = result['data']\n",
    "        \n",
    "        # æ•°å€¤ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š\n",
    "        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        \n",
    "        if len(numeric_cols) > 0 and len(df) > 1:\n",
    "            # æœ€åˆã®æ•°å€¤ã‚«ãƒ©ãƒ ã§ã‚°ãƒ©ãƒ•ä½œæˆ\n",
    "            if len(df) <= 20:  # 20è¡Œä»¥ä¸‹ãªã‚‰æ£’ã‚°ãƒ©ãƒ•\n",
    "                fig = px.bar(\n",
    "                    df.head(20), \n",
    "                    y=numeric_cols[0],\n",
    "                    x=df.index if df.index.name else range(len(df)),\n",
    "                    title=f\"{question}ã®çµæœ\"\n",
    "                )\n",
    "            else:  # ãã‚Œä»¥ä¸Šãªã‚‰ç·šã‚°ãƒ©ãƒ•\n",
    "                fig = px.line(\n",
    "                    df.head(100),\n",
    "                    y=numeric_cols[0],\n",
    "                    x=df.index if df.index.name else range(len(df)),\n",
    "                    title=f\"{question}ã®çµæœ\"\n",
    "                )\n",
    "            \n",
    "            fig.show()\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"åˆ†æé–¢æ•°ã®æº–å‚™å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. åˆ†æä¾‹ã®å®Ÿè¡Œ\n",
    "\n",
    "ä»¥ä¸‹ã®ã‚»ãƒ«ã§è‡ªç„¶è¨€èªã«ã‚ˆã‚‹åˆ†æã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æä¾‹1: å£²ä¸Šä¸Šä½ã®ä¼æ¥­\n",
    "result1 = run_analysis(\"å¹´é–“å£²ä¸ŠãŒæœ€ã‚‚é«˜ã„ä¸Šä½10ç¤¾ã‚’æ•™ãˆã¦\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æä¾‹2: å•†è«‡ã®åˆ†æ\n",
    "result2 = run_analysis(\"2024å¹´ã®ClosedWonã®å•†è«‡é‡‘é¡ã®åˆè¨ˆã‚’æœˆåˆ¥ã«é›†è¨ˆã—ã¦\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æä¾‹3: æ¥­ç•Œåˆ¥åˆ†æ\n",
    "result3 = run_analysis(\"æ¥­ç•Œåˆ¥ã®ä¼æ¥­æ•°ã¨å¹³å‡å¾“æ¥­å“¡æ•°ã‚’æ•™ãˆã¦\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æä¾‹4: å•†è«‡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
    "result4 = run_analysis(\"ç¾åœ¨é€²è¡Œä¸­ã®å•†è«‡ã‚’ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥ã«é‡‘é¡é›†è¨ˆã—ã¦\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æä¾‹5: é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æ\n",
    "result5 = run_analysis(\"ENTãƒ•ãƒ©ã‚°ãŒTrueã®ä¼æ¥­ã®å•†è«‡ã®å‹ç‡ã‚’è¨ˆç®—ã—ã¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªåˆ†æ\n",
    "\n",
    "è‡ªç”±ã«è³ªå•ã‚’å…¥åŠ›ã—ã¦åˆ†æã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‡ªç”±ã«è³ªå•ã‚’å…¥åŠ›\n",
    "question = input(\"åˆ†æã—ãŸã„å†…å®¹ã‚’è‡ªç„¶è¨€èªã§å…¥åŠ›ã—ã¦ãã ã•ã„: \")\n",
    "result = run_analysis(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. è¤‡æ•°è³ªå•ã®ä¸€æ‹¬åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¤‡æ•°ã®è³ªå•ã‚’ä¸€æ‹¬ã§åˆ†æ\n",
    "questions = [\n",
    "    \"MIDãƒ•ãƒ©ã‚°ãŒTrueã®ä¼æ¥­æ•°ã¯ï¼Ÿ\",\n",
    "    \"2024å¹´ã®æ–°è¦å•†è«‡æ•°ã‚’æœˆåˆ¥ã«é›†è¨ˆ\",\n",
    "    \"Company_FitãŒHighã®ä¼æ¥­ã®ç‰¹å¾´ã¯ï¼Ÿ\",\n",
    "    \"å•†è«‡ã®å¹³å‡ã‚¯ãƒ­ãƒ¼ã‚ºæœŸé–“ã¯ï¼Ÿ\",\n",
    "    \"åœ°åŸŸåˆ¥ã®ä¼æ¥­åˆ†å¸ƒã‚’æ•™ãˆã¦\"\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for q in questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"åˆ†æä¸­: {q}\")\n",
    "    print('='*60)\n",
    "    try:\n",
    "        results[q] = run_analysis(q, show_chart=False)\n",
    "    except Exception as e:\n",
    "        print(f\"ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        results[q] = None\n",
    "\n",
    "# çµæœã‚µãƒãƒªãƒ¼\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š å…¨ä½“ã‚µãƒãƒªãƒ¼\")\n",
    "print(\"=\"*60)\n",
    "for q, r in results.items():\n",
    "    if r:\n",
    "        print(f\"\\nã€{q}ã€‘\")\n",
    "        print(r['summary'][:200] + \"...\" if len(r['summary']) > 200 else r['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. çµæœã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æçµæœã‚’CSVã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "def export_results(result_dict, filename=\"analysis_result.csv\"):\n",
    "    \"\"\"åˆ†æçµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\"\"\"\n",
    "    if result_dict and 'data' in result_dict and not result_dict['data'].empty:\n",
    "        # DataFrameã‚’CSVã«å¤‰æ›\n",
    "        result_dict['data'].to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"âœ… {filename} ã‚’ä½œæˆã—ã¾ã—ãŸ\")\n",
    "        \n",
    "        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "        files.download(filename)\n",
    "    else:\n",
    "        print(\"ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "\n",
    "# æœ€å¾Œã®åˆ†æçµæœã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\n",
    "if 'result' in locals():\n",
    "    export_results(result, \"last_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ã‚ˆãä½¿ã†åˆ†æãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚ˆãä½¿ã†åˆ†æã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ\n",
    "analysis_templates = {\n",
    "    \"å–¶æ¥­ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹\": [\n",
    "        \"ä»Šæœˆã®ClosedWonå•†è«‡ã®åˆè¨ˆé‡‘é¡ã¯ï¼Ÿ\",\n",
    "        \"å–¶æ¥­æ‹…å½“è€…åˆ¥ã®å•†è«‡æˆç´„ç‡ã‚’è¨ˆç®—ã—ã¦\",\n",
    "        \"å¹³å‡å•†è«‡æœŸé–“ã‚’ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥ã«é›†è¨ˆ\"\n",
    "    ],\n",
    "    \"é¡§å®¢åˆ†æ\": [\n",
    "        \"å¾“æ¥­å“¡æ•°1000äººä»¥ä¸Šã®å¤§ä¼æ¥­ãƒªã‚¹ãƒˆã‚’è¡¨ç¤º\",\n",
    "        \"éå»1å¹´é–“ã§æœ€ã‚‚å–å¼•é‡‘é¡ãŒå¤šã„ä¸Šä½20ç¤¾\",\n",
    "        \"æ¥­ç•Œåˆ¥ã®é¡§å®¢åˆ†å¸ƒã¨å¹³å‡å–å¼•é¡\"\n",
    "    ],\n",
    "    \"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆ†æ\": [\n",
    "        \"ç¾åœ¨ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç·é¡ã‚’ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥ã«è¡¨ç¤º\",\n",
    "        \"ä»Šå››åŠæœŸã«ã‚¯ãƒ­ãƒ¼ã‚ºäºˆå®šã®å•†è«‡ãƒªã‚¹ãƒˆ\",\n",
    "        \"å•†è«‡ã®å‹ç‡ã‚’é‡‘é¡ãƒ¬ãƒ³ã‚¸åˆ¥ã«åˆ†æ\"\n",
    "    ],\n",
    "    \"ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ\": [\n",
    "        \"éå»12ãƒ¶æœˆã®æ–°è¦å•†è«‡æ•°ã®æ¨ç§»\",\n",
    "        \"æœˆåˆ¥ã®å—æ³¨é‡‘é¡ãƒˆãƒ¬ãƒ³ãƒ‰\",\n",
    "        \"æ–°è¦é¡§å®¢ç²å¾—æ•°ã®å››åŠæœŸåˆ¥æ¨ç§»\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé¸æŠ\n",
    "print(\"åˆ©ç”¨å¯èƒ½ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:\")\n",
    "for i, category in enumerate(analysis_templates.keys(), 1):\n",
    "    print(f\"{i}. {category}\")\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªé¸æŠ\n",
    "category_num = int(input(\"\\nã‚«ãƒ†ã‚´ãƒªç•ªå·ã‚’é¸æŠ (1-4): \"))\n",
    "selected_category = list(analysis_templates.keys())[category_num - 1]\n",
    "\n",
    "print(f\"\\n{selected_category}ã®åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# é¸æŠã—ãŸã‚«ãƒ†ã‚´ãƒªã®åˆ†æã‚’å®Ÿè¡Œ\n",
    "for question in analysis_templates[selected_category]:\n",
    "    print(f\"\\nâ–¶ {question}\")\n",
    "    try:\n",
    "        result = run_analysis(question, show_chart=True)\n",
    "    except Exception as e:\n",
    "        print(f\"ã‚¨ãƒ©ãƒ¼: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. ã‚«ã‚¹ã‚¿ãƒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "\n",
    "def create_dashboard():\n",
    "    \"\"\"è¤‡æ•°ã®åˆ†æçµæœã‚’çµ„ã¿åˆã‚ã›ãŸãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’ä½œæˆ\"\"\"\n",
    "    \n",
    "    # 4ã¤ã®ä¸»è¦åˆ†æã‚’å®Ÿè¡Œ\n",
    "    analyses = {\n",
    "        \"æœˆåˆ¥å£²ä¸Š\": \"2024å¹´ã®æœˆåˆ¥å—æ³¨é‡‘é¡ã‚’é›†è¨ˆ\",\n",
    "        \"ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥å•†è«‡\": \"ç¾åœ¨ã®å•†è«‡ã‚’ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥ã«ä»¶æ•°é›†è¨ˆ\",\n",
    "        \"æ¥­ç•Œåˆ¥é¡§å®¢\": \"æ¥­ç•Œåˆ¥ã®é¡§å®¢æ•°ã‚’ä¸Šä½10ä»¶\",\n",
    "        \"å•†è«‡å‹ç‡\": \"éå»6ãƒ¶æœˆã®æœˆåˆ¥å•†è«‡å‹ç‡\"\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for title, query in analyses.items():\n",
    "        try:\n",
    "            print(f\"åˆ†æä¸­: {title}\")\n",
    "            results[title] = analyzer.analyze(query)\n",
    "        except Exception as e:\n",
    "            print(f\"ã‚¨ãƒ©ãƒ¼ ({title}): {e}\")\n",
    "            results[title] = None\n",
    "    \n",
    "    # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ\n",
    "    fig = sp.make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=list(analyses.keys()),\n",
    "        specs=[\n",
    "            [{'type': 'bar'}, {'type': 'bar'}],\n",
    "            [{'type': 'bar'}, {'type': 'scatter'}]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # å„ã‚°ãƒ©ãƒ•ã‚’è¿½åŠ \n",
    "    positions = [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
    "    \n",
    "    for (title, result), (row, col) in zip(results.items(), positions):\n",
    "        if result and not result['data'].empty:\n",
    "            df = result['data'].head(20)\n",
    "            \n",
    "            # æœ€åˆã®ã‚«ãƒ©ãƒ ã‚’Xè»¸ã€2ç•ªç›®ã®æ•°å€¤ã‚«ãƒ©ãƒ ã‚’Yè»¸ã¨ã—ã¦ä½¿ç”¨\n",
    "            if len(df.columns) >= 2:\n",
    "                x_col = df.columns[0]\n",
    "                \n",
    "                # æ•°å€¤ã‚«ãƒ©ãƒ ã‚’æ¢ã™\n",
    "                numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "                if len(numeric_cols) > 0:\n",
    "                    y_col = numeric_cols[0]\n",
    "                    \n",
    "                    if row == 2 and col == 2:  # æœ€å¾Œã¯ç·šã‚°ãƒ©ãƒ•\n",
    "                        fig.add_trace(\n",
    "                            go.Scatter(\n",
    "                                x=df[x_col].astype(str),\n",
    "                                y=df[y_col],\n",
    "                                mode='lines+markers',\n",
    "                                name=title\n",
    "                            ),\n",
    "                            row=row, col=col\n",
    "                        )\n",
    "                    else:  # ãã®ä»–ã¯æ£’ã‚°ãƒ©ãƒ•\n",
    "                        fig.add_trace(\n",
    "                            go.Bar(\n",
    "                                x=df[x_col].astype(str),\n",
    "                                y=df[y_col],\n",
    "                                name=title\n",
    "                            ),\n",
    "                            row=row, col=col\n",
    "                        )\n",
    "    \n",
    "    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨­å®š\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        showlegend=False,\n",
    "        title_text=\"Salesforceãƒ‡ãƒ¼ã‚¿åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰\",\n",
    "        title_font_size=20\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ\n",
    "dashboard_results = create_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. åˆ†æå±¥æ­´ã®ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æå±¥æ­´ã‚’ä¿å­˜\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# åˆ†æå±¥æ­´ã‚¯ãƒ©ã‚¹\n",
    "class AnalysisHistory:\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "    \n",
    "    def add(self, query, sql, result_shape, summary):\n",
    "        self.history.append({\n",
    "            'timestamp': datetime.now(),\n",
    "            'query': query,\n",
    "            'sql': sql,\n",
    "            'result_shape': result_shape,\n",
    "            'summary': summary\n",
    "        })\n",
    "    \n",
    "    def show_history(self, last_n=10):\n",
    "        \"\"\"æœ€è¿‘ã®åˆ†æå±¥æ­´ã‚’è¡¨ç¤º\"\"\"\n",
    "        for item in self.history[-last_n:]:\n",
    "            print(f\"\\nğŸ“… {item['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(f\"â“ è³ªå•: {item['query']}\")\n",
    "            print(f\"ğŸ“Š çµæœ: {item['result_shape']}\")\n",
    "            print(f\"ğŸ“ ã‚µãƒãƒªãƒ¼: {item['summary'][:100]}...\")\n",
    "    \n",
    "    def save(self, filename='analysis_history.pkl'):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self.history, f)\n",
    "        print(f\"å±¥æ­´ã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "    \n",
    "    def load(self, filename='analysis_history.pkl'):\n",
    "        try:\n",
    "            with open(filename, 'rb') as f:\n",
    "                self.history = pickle.load(f)\n",
    "            print(f\"å±¥æ­´ã‚’ {filename} ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸ\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "\n",
    "# å±¥æ­´ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ\n",
    "history = AnalysisHistory()\n",
    "\n",
    "# ã“ã‚Œã¾ã§ã®åˆ†æã‚’å±¥æ­´ã«è¿½åŠ ï¼ˆã‚‚ã—çµæœãŒã‚ã‚Œã°ï¼‰\n",
    "if 'results' in locals():\n",
    "    for q, r in results.items():\n",
    "        if r:\n",
    "            history.add(\n",
    "                query=r['query'],\n",
    "                sql=r['sql'],\n",
    "                result_shape=str(r['data'].shape) if 'data' in r else 'N/A',\n",
    "                summary=r.get('summary', 'N/A')\n",
    "            )\n",
    "\n",
    "# å±¥æ­´è¡¨ç¤º\n",
    "print(\"=== åˆ†æå±¥æ­´ ===\")\n",
    "history.show_history()\n",
    "\n",
    "# å±¥æ­´ä¿å­˜\n",
    "history.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã¾ã¨ã‚\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¦ã„ã¾ã™ï¼š\n",
    "\n",
    "1. **è‡ªç„¶è¨€èªã§ã®ã‚¯ã‚¨ãƒª**: æ—¥æœ¬èªã§ãƒ‡ãƒ¼ã‚¿ã«é–¢ã™ã‚‹è³ªå•ã‚’ã™ã‚‹ã ã‘ã§åˆ†æå¯èƒ½\n",
    "2. **è‡ªå‹•SQLç”Ÿæˆ**: Gemini APIãŒã‚¹ã‚­ãƒ¼ãƒã‚’ç†è§£ã—ã¦SQLã‚’ç”Ÿæˆ\n",
    "3. **çµæœã®å¯è¦–åŒ–**: Plotlyã«ã‚ˆã‚‹è‡ªå‹•ã‚°ãƒ©ãƒ•ç”Ÿæˆ\n",
    "4. **ã‚µãƒãƒªãƒ¼ç”Ÿæˆ**: AIã«ã‚ˆã‚‹åˆ†æçµæœã®è¦ç´„\n",
    "5. **ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**: è¤‡æ•°åˆ†æã®çµ±åˆè¡¨ç¤º\n",
    "6. **å±¥æ­´ç®¡ç†**: åˆ†æå±¥æ­´ã®ä¿å­˜ã¨å‚ç…§\n",
    "\n",
    "### ä½¿ã„æ–¹ã®ãƒ’ãƒ³ãƒˆ\n",
    "\n",
    "- å…·ä½“çš„ãªæ•°å€¤ã‚„æœŸé–“ã‚’å«ã‚ã‚‹ã¨ã€ã‚ˆã‚Šæ­£ç¢ºãªåˆ†æãŒã§ãã¾ã™\n",
    "- ã€Œä¸Šä½10ä»¶ã€ã€Œæœˆåˆ¥ã€ã€Œæ¥­ç•Œåˆ¥ã€ãªã©ã®é›†è¨ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ã†ã¨åŠ¹æœçš„ã§ã™\n",
    "- è¤‡é›‘ãªåˆ†æã¯æ®µéšçš„ã«å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™\n",
    "\n",
    "### æ³¨æ„äº‹é …\n",
    "\n",
    "- å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ã‚¨ãƒªã¯ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹ãŸã‚ã€LIMITå¥ãŒè‡ªå‹•è¿½åŠ ã•ã‚Œã¾ã™\n",
    "- Gemini APIã®åˆ©ç”¨åˆ¶é™ã«æ³¨æ„ã—ã¦ãã ã•ã„\n",
    "- æ©Ÿå¯†æƒ…å ±ã®å–ã‚Šæ‰±ã„ã«ã¯ååˆ†æ³¨æ„ã—ã¦ãã ã•ã„"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}